---
title: "DataProc - Understanding environments: shared vs local"
slug: "dataproc_environments"
date: 2025-02-15T16:00:00-05:00
publishDate: 2025-02-15T16:00:00-05:00
author: "Morgan Bye"
draft: false

tags: ["Technology"]
toc: true
description: "An explainer on how Google Cloud Composer and DataProc environments interact with each and share resources."
---

This post is part of a comprehensive series to DataProc and Airflow. You can read everything in this [one monster piece](https://morganbye.com/posts/dataproc/), or you can jump to a particular section

1. [What is DataProc? And why do we need it?](https://morganbye.com/posts/dataproc_overview/)
2. [Environments, what's shared and what's local](https://morganbye.com/posts/dataproc_environments/)
3. [Environment variables](https://morganbye.com/posts/dataproc_variables/)
4. [Cluster configuration](https://morganbye.com/posts/dataproc_cluster_configuration/)
5. [Cluster startup optimization](https://morganbye.com/posts/dataproc_startup_optimization/)
6. [Runtime variables](https://morganbye.com/posts/dataproc_runtime_variables/)

---

# On this page

{{< toc >}}

---

# Understanding environments: shared vs local

## Conceptual overview of Airflow and Spark environments

[**Google Cloud Composer**](https://cloud.google.com/composer?hl=en), a managed [**Apache Airflow**](https://airflow.apache.org/), orchestrates workflows using [**Directed Acyclic Graphs (DAGs)**](https://www.geeksforgeeks.org/introduction-to-directed-acyclic-graph/). DAGs define a sequence of tasks and their dependencies, ensuring reliable workflow execution.

A typical **Composer DAG** follows three primary steps:

1. **Provisioning a DataProc cluster**
2. **Executing the workload**
3. **Tearing down the cluster**

This structure ensures efficient resource utilization and cost optimization.

{{< image path="2025/1_airflow_sequence.png" alt="Typical Airflow training job" >}}

## How Airflow and DataProc work together

A [**DataProc cluster**](https://cloud.google.com/dataproc?hl=en) is a Google Cloud-managed service that transforms **Compute Instance resources** into an [**Apache Spark**](https://spark.apache.org/) cluster for processing large-scale data. When Airflow provisions a cluster, it effectively creates a **secondary execution environment** within a **dedicated Virtual Private Cloud (VPC)**.

{{< image path="2025/2_provision_dataproc.png" alt="Relationship between Airflow and DataProc environments">}}

## Configuring the DataProc environment

DataProc clusters can be provisioned using:

- **Google Cloud Console**
- **Command-line tools**
- **Airflow's DataProcClusterOperator**

At cluster creation, several background operations are automatically executed, including:

- Provisioning **head node(s)**
- Installing the **Hadoop Distributed File System (HDFS)**
- Configuring **YARN** for resource scheduling
- Establishing **internal networking**

{{< image path="2025/3_start_spark_cluster.png" >}}

## Provisioning worker nodes

Once the Spark cluster is initialized, worker nodes are dynamically provisioned based on the specified configuration. Each worker node:

1. Pulls the **base DataProc image**
2. Starts the **Spark service**
3. Runs **initialization scripts**

This process repeats for each requested worker node.

{{< image path="2025/4_provision_workers.png" >}}

Worker nodes undergo health checks before registering with **head nodes** to receive jobs.

## Executing PySpark Jobs

Within the cluster, **Python code** runs as **PySpark jobs** on worker nodes. Each worker node requires a **Python environment**, ensuring compatibility with submitted scripts. Once all jobs are processed, Airflow enters the **Teardown phase**, deallocating resources.

{{< image path="2025/5_submit_python_job.png" >}}

## Configuration scope and security considerations

When tuning an Airflow-Spark system, it's essential to recognize that **settings and parameters exist at multiple levels**. Each layer—including Airflow, DataProc, Spark, and individual worker nodes—can influence execution performance. Additionally, **security policies** can be enforced at various levels, affecting deployment strategies and access control.

{{< image path="2025/6_config_layers.png" >}}

## Conclusion

Understanding **shared and local environments** within Airflow and DataProc is essential for designing scalable, cost-effective workflows. By correctly configuring clusters, defining resource scopes, and implementing security policies, teams can optimize performance while maintaining a robust orchestration framework.
