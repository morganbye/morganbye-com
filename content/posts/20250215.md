---
title: "DataProc - Understanding environments: shared vs local"
slug: "dataproc_environments"
date: 2025-02-15T16:00:00-05:00
publishDate: 2025-02-15T16:00:00-05:00
author: "Morgan Bye"
draft: false

tags: ["Technology"]
toc: true
description: "An explainer on how Google Cloud Composer and DataProc environments interact with each and share resources."
---

This post is part of a comprehensive series to DataProc and Airflow
1. [Everything you need to know (overview)](https://morganbye.com/posts/dataproc_overview/)
2. [Environments, what's shared and what's local](https://morganbye.com/posts/dataproc_environments/)

## Conceptual Overview of Airflow and Spark Environments

**Google Cloud Composer**, a managed **Apache Airflow** service, orchestrates workflows using **Directed Acyclic Graphs (DAGs)**. DAGs define a sequence of tasks and their dependencies, ensuring reliable workflow execution.

A typical **Composer DAG** follows three primary steps:

1. **Provisioning a DataProc cluster**
2. **Executing the workload**
3. **Tearing down the cluster**

This structure ensures efficient resource utilization and cost optimization.

{{< image path="2025/1_airflow_sequence.png" alt="Typical Airflow training job" >}}

## How Airflow and DataProc Work Together

A **DataProc cluster** is a Google Cloud-managed service that transforms **Compute Instance resources** into an **Apache Spark** cluster for processing large-scale data. When Airflow provisions a cluster, it effectively creates a **secondary execution environment** within a **dedicated Virtual Private Cloud (VPC)**.

{{< image path="2025/2_provision_dataproc.png" alt="Relationship between Airflow and DataProc environments">}}

## Configuring the DataProc Environment

DataProc clusters can be provisioned using:
- **Google Cloud Console**
- **Command-line tools**
- **Airflow's DataProcClusterOperator**

At cluster creation, several background operations are automatically executed, including:
- Provisioning **head node(s)**
- Installing the **Hadoop Distributed File System (HDFS)**
- Configuring **YARN** for resource scheduling
- Establishing **internal networking**

{{< image path="2025/3_start_spark_cluster.png" >}}

## Provisioning Worker Nodes

Once the Spark cluster is initialized, worker nodes are dynamically provisioned based on the specified configuration. Each worker node:

1. Pulls the **base DataProc image**
2. Starts the **Spark service**
3. Runs **initialization scripts**

This process repeats for each requested worker node.

{{< image path="2025/4_provision_workers.png" >}}

Worker nodes undergo health checks before registering with **head nodes** to receive jobs.

## Executing PySpark Jobs

Within the cluster, **Python code** runs as **PySpark jobs** on worker nodes. Each worker node requires a **Python environment**, ensuring compatibility with submitted scripts. Once all jobs are processed, Airflow enters the **Teardown phase**, deallocating resources.

{{< image path="2025/5_submit_python_job.png" >}}

## Configuration Scope and Security Considerations

When tuning an Airflow-Spark system, it's essential to recognize that **settings and parameters exist at multiple levels**. Each layer—including Airflow, DataProc, Spark, and individual worker nodes—can influence execution performance. Additionally, **security policies** can be enforced at various levels, affecting deployment strategies and access control.

{{< image path="2025/6_config_layers.png" >}}

## Conclusion

Understanding **shared and local environments** within Airflow and DataProc is essential for designing scalable, cost-effective workflows. By correctly configuring clusters, defining resource scopes, and implementing security policies, teams can optimize performance while maintaining a robust orchestration framework.
