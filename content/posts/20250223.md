---
title: "DataProc - Passing runtime variables"
slug: "dataproc_runtime_variables"
date: 2025-02-23T16:00:00-05:00
publishDate: 2025-02-23T16:00:00-05:00
author: "Morgan Bye"
draft: false

tags: ["Technology"]
toc: true
description: "An explainer on how to pass runtime variables to Google Cloud DataProc for Python AI and ML applications."
---

This post is part of a comprehensive series to DataProc and Airflow. You can read everything in this [one monster piece](https://morganbye.com/posts/dataproc/), or you can jump to a particular section

1. [What is DataProc? And why do we need it?](https://morganbye.com/posts/dataproc_overview/)
2. [Environments, what's shared and what's local](https://morganbye.com/posts/dataproc_environments/)
3. [Environment variables](https://morganbye.com/posts/dataproc_variables/)
4. [Cluster configuration](https://morganbye.com/posts/dataproc_cluster_configuration/)
5. [Cluster startup optimization](https://morganbye.com/posts/dataproc_startup_optimization/)
6. [Runtime variables](https://morganbye.com/posts/dataproc_runtime_variables/)

---

# On this page

{{< toc >}}

---

# Passing runtime variables in DataProc and Airflow

## Introduction

Using environment variables to pass secrets and configurations at runtime is a **DevOps best practice** that enhances security, flexibility, and maintainability. Hard-coding sensitive values in source code increases the risk of leaks, while environment variables keep secrets out of version control systems. This also enables **dynamic configuration**, allowing jobs to adapt across different environments (e.g., DEV, UAT, PRD) without modifying the application code.

In DataProc, variables can be set within **Google Cloud Composer** and injected into DataProc worker environments, allowing job scripts to retrieve these values dynamically.

---

## Methods for passing runtime variables

Apache Spark provides multiple ways to pass variables to the runtime environment. The following are the four primary approaches, listed in order of preference.

### OS-level initialization script (cluster startup)

Google Cloudâ€™s recommended method is defining a **startup script** during cluster creation. This script is stored in **Cloud Storage** and executed as an [**initialization action**](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/init-actions).

#### Example command

```bash
gcloud dataproc clusters create <NAME> \
    --region=${REGION} \
    --initialization-actions=gs://<bucket>/startup.sh \
    --initialization-action-timeout=10m \
    ... other flags ...
```

Using this method, environment variables can be **appended to the system's [`/etc/environment`](https://spark.apache.org/docs/latest/configuration.html#environment-variables) file** which is run as the final initialization step for every machine that is added to the cluster.

#### Example startup script

```bash
#!/usr/bin/env bash

echo "FOO=BAR" >> /etc/environment
```

---

### Compute engine metadata (instance-level variables)

Since DataProc clusters are built on **Google Compute Engine (GCE) instances**, [metadata can be passed to both **head** and **worker nodes** at time of provisioning](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/metadata) from Google management layer.

#### Example command

```bash
gcloud dataproc clusters \
    create <NAME> \
    --metadata foo=BAR,startup-script-url=gs://<bucket>/startup.sh
```

Metadata can also be used for [**Hadoop and Spark properties**](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/cluster-properties):

```bash
gcloud dataproc clusters \
    create <NAME> \
    --properties hadoop-env:FOO=hello,spark-env:BAR=world
```

---

### Spark properties (job submission-level variables)

If the environment variables need to be **job-specific**, they can be injected directly into the [**Spark runtime environment**](https://spark.apache.org/docs/latest/configuration.html) when submitting a job.

#### Example command

```bash
gcloud dataproc jobs \
    submit spark \
    --properties spark.executorEnv.FOO=world
```

From **Airflow**, the [`DataprocSubmitJobOperator`](https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/dataproc/index.html#airflow.providers.google.cloud.operators.dataproc.DataprocSubmitJobOperator) supports passing **job-level properties**:

```python
DataprocSubmitJobOperator(
    task_id="example_task",
    region=REGION,
    project_id=PROJECT,
    job={
        "placement": {"cluster_name": CLUSTER_NAME},
        "pyspark_job": {
            "main_python_file_uri": f"gs://{BUCKET}/dags/predict.py",
            "python_file_uris": [PKG],
            "args": [
                "--store_key", store,
                "--start_date", PARAMS.start_date.strftime("%Y-%m-%d %H:%M"),
                "--end_date", PARAMS.end_date.strftime("%Y-%m-%d %H:%M"),
            ],
            "properties": {
                "ENV": ENV,
                "PROJECT": PROJECT,
                "REGION": REGION,
                "BUCKET": BUCKET,
            },
        },
    },
)
```

> [!WARNING]
> Some security policies may block custom environment variables from being passed using this method.

---

### Command-line arguments (explicit variable passing)

If other methods fail due to **security restrictions**, environment variables can be passed explicitly as **command-line arguments**.

```python
DataprocSubmitJobOperator(
    task_id="example_task",
    region=REGION,
    project_id=PROJECT,
    job={
        "placement": {"cluster_name": CLUSTER_NAME},
        "pyspark_job": {
            "main_python_file_uri": f"gs://{BUCKET}/dags/predict.py",
            "python_file_uris": [PKG],
            "args": [
                "--site", site,
                "--start_date", PARAMS.start_date.strftime("%Y-%m-%d %H:%M"),
                "--end_date", PARAMS.end_date.strftime("%Y-%m-%d %H:%M"),
                "--env",  # NEW ARGUMENT ADDED
                ENV,
            ],
        },
    },
)
```

This requires modifying the **entry point** of the run script to accept the new argument:

```python
parser.add_argument("--env", required=False, default="local", help="Runtime environment.")
```

---

## Use a secrets manager

The **most secure** way to manage runtime variables is to store sensitive values in a **secrets manager** rather than passing them directly.

### Why Use a Secrets Manager?

- **Security**: Keeps secrets out of logs, DAGs, and environment variables.  
- **Access Control**: Secrets can be **role-restricted** to prevent unauthorized access.  
- **Versioning**: Allows tracking changes to secrets over time.  
- **Auditing**: Provides logging to track access attempts.
- **Ease of coding**: The same variables can be used across deployment environments, so long as each environment has its own Secret Manager.

### Google Cloud Secret Manager

Google Cloud Secret Manager provides **centralized, access-controlled storage** for secrets. All we need to do is add Secret Manager access to the deployment Service Account and then we can replace most variables with a simple secret lookup.

#### 1) Store a Secret

```bash
gcloud secrets create MY_SECRET --replication-policy="automatic"
```

#### 2) Set a value

```bash
echo -n "my_secret_value" | gcloud secrets versions add MY_SECRET --data-file=-
```

#### 3) Retrieve a secret from Airflow

In Airflow DAGs, secrets can be accessed using **Google Secret Manager Hook**:

```python
from airflow.providers.google.cloud.hooks.secret_manager import SecretManagerHook

def get_secret(secret_name: str):
    hook = SecretManagerHook()
    return hook.get_secret(secret_name)

MY_SECRET_VALUE = get_secret("MY_SECRET")
```

#### 4) Retrieve a Secret from DataProc

Alternatively, secrets can be accessed directly from within the DataProc environment using the [Google Cloud Secret Manager Python client](https://cloud.google.com/python/docs/reference/secretmanager/latest).

Ensure that the Secret Manager library is [included in the project requirements and installed during project initialization](https://morganbye.com/posts/dataproc_startup_optimization/)

Then in your PySpark job, retrieve the Secret

```python
from google.cloud import secretmanager

def get_secret(secret_name: str, project: str) -> str:
    """Retrieve a secret from Google Cloud Secret Manager."""
    client = secretmanager.SecretManagerServiceClient()
    secret_path = f"projects/{project}/secrets/{secret_name}/versions/latest"
    response = client.access_secret_version(request={"name": secret_path})
    return response.payload.data.decode("UTF-8")

# Example usage
SECRET_NAME = "MY_SECRET"

secret_value = get_secret(SECRET_NAME, PROJECT)
```

This method has the advantage of only pulling secrets when they are needed at runtime rather than at DAG execution time - this limits their scope to a single function or class rather than the entire execution environment.


---

## Conclusion

When configuring runtime variables in DataProc and Airflow, the best method depends on **security policies** and **operational requirements**:

| Method | Use Case |
|--------|----------|
| **OS-Level Startup Script** | Best for cluster-wide persistent variables |
| **Compute Engine Metadata** | Works at the instance level; useful for per-node settings |
| **Spark Properties** | Best for job-specific runtime variables |
| **Command-Line Arguments** | A fallback option when security policies restrict variable injection |
| **Secrets Manager** | The **most secure** option for sensitive values |

For sensitive data such as **API keys, database credentials, and encryption secrets**, **Google Cloud Secret Manager** should always be used.

This structured approach ensures secure and flexible runtime configuration across different environments.
